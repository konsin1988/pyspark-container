{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96fa2eaa-631a-45e6-afe0-47aafd57060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import optuna\n",
    "\n",
    "# from giskard import Dataset, Model, scan, testing\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "\n",
    "import clickhouse_connect\n",
    "import psycopg2\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import clickhouse_connect\n",
    "\n",
    "import mlflow \n",
    "import mlflow.catboost\n",
    "import mlflow.sklearn\n",
    "import mlflow.data\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0affb1d2-a2b0-43a8-a436-d741237db1ba",
   "metadata": {},
   "source": [
    "# Load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabb5adc-d04c-472f-9f2f-237196ac3bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SPARK_COMPAT_VERSION = os.getenv('SPARK_COMPAT_VERSION')\n",
    "SCALA_COMPAT_VERSION = os.getenv('SCALA_COMPAT_VERSION')\n",
    "CATBOOST_SPARK_VERSION = os.getenv('CATBOOST_SPARK_VERSION')\n",
    "CLICKHOUSE_HOST = os.getenv('CLICKHOUSE_HOST')\n",
    "CLICKHOUSE_PORT = os.getenv('CLICKHOUSE_PORT')\n",
    "CLICKHOUSE_USER = os.getenv('CLICKHOUSE_USER')\n",
    "CLICKHOUSE_PASSWORD = os.getenv('CLICKHOUSE_PASSWORD')\n",
    "\n",
    "ACCESS_KEY=os.getenv('MINIO_ROOT_USER')\n",
    "SECRET_KEY=os.getenv('MINIO_ROOT_PASSWORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635de937-0c9b-4c0e-bcda-a63570ea3219",
   "metadata": {},
   "source": [
    "# Get spark client, connect to clickhouse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c972dd8e-38c3-49b8-88c9-dec39c9f3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "packages = [\n",
    "    \"com.clickhouse.spark:clickhouse-spark-runtime-3.4_2.12:0.8.0\",\n",
    "    \"com.clickhouse:clickhouse-client:0.7.0\",\n",
    "    \"com.clickhouse:clickhouse-http-client:0.7.0\",\n",
    "    \"org.apache.httpcomponents.client5:httpclient5:5.2.1\",\n",
    "    f\"ai.catboost:catboost-spark_{SPARK_COMPAT_VERSION}_{SCALA_COMPAT_VERSION}:{CATBOOST_SPARK_VERSION}\"\n",
    "]\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(\"spark.jars.packages\", \",\".join(packages))\n",
    "         .master(\"local[1]\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse\", \"com.clickhouse.spark.ClickHouseCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse.host\", \"127.0.0.1\")\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse.user\", \"konsin1988\")\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse.password\", \"r13l02c1988\")\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse.database\", \"credit\")\n",
    "spark.conf.set(\"spark.clickhouse.write.format\", \"json\")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize(range(100 + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b2538-c219-4fa9-88c3-6a31b0ad4042",
   "metadata": {},
   "source": [
    "### Import catboost_spark (only after spark packages loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b64eda-662f-4ff0-8e39-76144cdc66b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost_spark\n",
    "from catboost_spark import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba60c0c-ae4b-49cd-a267-91d83b999a5f",
   "metadata": {},
   "source": [
    "# Set feature and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43bbaa73-71ae-44ea-acfe-e4af9ee62f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_TYPES = {\n",
    "    'age': 'category',\n",
    "    'sex': 'category',\n",
    "    'job': 'category',\n",
    "    'housing': 'category',\n",
    "    'credit_amount': 'numeric',\n",
    "    'duration': 'numeric'\n",
    "}\n",
    "\n",
    "TARGET_COLUMN_NAME = 'default'\n",
    "FEATURE_COLUMNS = [i for i in COLUMN_TYPES.keys()]\n",
    "FEATURE_TYPES = {i: COLUMN_TYPES[i] for i in COLUMN_TYPES if i != TARGET_COLUMN_NAME}\n",
    "\n",
    "COLUMNS_TO_SCALE = [key for key in COLUMN_TYPES.keys() if COLUMN_TYPES[key] == \"numeric\"]\n",
    "COLUMNS_TO_ENCODE = [key for key in COLUMN_TYPES.keys() if COLUMN_TYPES[key] == \"category\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34845d40-1698-4ee5-99ac-a56d67678362",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d864ee-238e-4568-843a-e85380ac235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list = {\n",
    "    0: 'unskilled and non-resident', \n",
    "    1: 'unskilled and resident', \n",
    "    2: 'skilled', \n",
    "    3: 'highly skilled'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5b1a7-3da2-47c2-9772-b4e6d9c3a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_query = f\"SELECT {reduce(lambda a,b: a + ', ' + b, FEATURE_COLUMNS)} FROM clickhouse.credit.credit;\"\n",
    "\n",
    "str_job = F.udf(lambda x: job_list[x], T.StringType())\n",
    "X = (\n",
    "    spark\n",
    "    .sql(X_query)\n",
    "    .withColumn('job', str_job(col('job')))\n",
    ")\n",
    "\n",
    "y_query = f'SELECT {TARGET_COLUMN_NAME} FROM clickhouse.credit.credit;'\n",
    "\n",
    "str_job = F.udf(lambda x: job_list[x], T.StringType())\n",
    "y = (\n",
    "    spark\n",
    "    .sql(y_query)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9514136-5050-45f1-9ae9-a40ec1d0654d",
   "metadata": {},
   "source": [
    "# MLflow connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50a167bf-2be4-412e-846a-bee076a7c50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI http://localhost:5000/\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "mlflow.set_tracking_uri('http://localhost:5000/')\n",
    "print(\"URI\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e59eb94-2dce-404a-8501-0139efa59824",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=DataFrame[age: bigint, sex: string, job: string, housing: string, credit_amount: bigint, duration: bigint].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m     10\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[1;32m     11\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     12\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m, numeric_transformer, COLUMNS_TO_SCALE),\n\u001b[1;32m     13\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, categorical_transformer, COLUMNS_TO_ENCODE)\n\u001b[1;32m     14\u001b[0m     ]\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m X_preproccessed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/compose/_column_transformer.py:900\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    897\u001b[0m _raise_for_params(params, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 900\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# set n_features_in_ attribute\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/compose/_column_transformer.py:1227\u001b[0m, in \u001b[0;36m_check_X\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dataframe__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n\u001b[0;32m-> 1227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/utils/validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_2d:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# If input is scalar raise error\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1012\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1013\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got scalar array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1014\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1015\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1016\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1017\u001b[0m         )\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;66;03m# If input is a Series-like object (eg. pandas Series or polars Series)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=DataFrame[age: bigint, sex: string, job: string, housing: string, credit_amount: bigint, duration: bigint].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "numeric_transformer = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "# \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, COLUMNS_TO_SCALE),\n",
    "        (\"cat\", categorical_transformer, COLUMNS_TO_ENCODE)\n",
    "    ]\n",
    ")\n",
    "X_preproccessed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7311993-0e40-42b9-9769-b05b5f1e188d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
